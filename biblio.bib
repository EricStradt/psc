%% LaTeX2e file `biblio.bib'
%% generated by the `filecontents' environment
%% from source `proposition' on 2014/09/22.
%%

@article{elhadad_natural_2010,
 title = {Natural Language Processing with Python},
 volume = {36},
 issn = {0891-2017},
 doi = {10.1162/coli_r_00022},
 language = {English},
 number = {4},
 journal = {Computational Linguistics},
 author = {Elhadad, Michael},
 month = dec,
 year = {2010},
 note = {{WOS}:000285382400009},
 pages = {767--771}
}

@article{fattah_ga_2009,
 title = {{GA}, {MR}, {FFNN}, {PNN} and {GMM} based models for automatic text summarization},
 volume = {23},
 issn = {0885-2308},
 doi = {10.1016/j.csl.2008.04.002},
 abstract = {This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality, sentence resemblance to the title, sentence inclusion of name entity, sentence inclusion of numerical data, sentence relative length, Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. First, we investigate the effect of cacti sentence feature on the summarization task. Then we use all features in combination to train genetic algorithm ({GA}) and mathematical regression ({MR}) models to obtain a suitable combination of feature weights. Moreover, we use all feature parameters to train feed forward neural network ({FFNN}), probabilistic neural network ({PNN}) and Gaussian mixture model ({GMM}) in order to construct a text summarizer for each model. Furthermore, we use trained models by one language to test summarization performance in the other language. The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles. The results of the proposed approach are promising, especially the {GMM} approach. (C) 2008 Elsevier Ltd. All rights reserved.},
 language = {English},
 number = {1},
 journal = {Computer Speech and Language},
 author = {Fattah, Mohamed Abdel and Ren, Fuji},
 month = jan,
 year = {2009},
 note = {{WOS}:000262688100007},
 keywords = {Automatic summarization, Feed forward neural network, Gaussian   mixture model, Genetic algorithm, latent semantic analysis, Mathematical regression, Probabilistic neural network, random-fields, sentence compression, speaker identification, trainable summarizer, verification},
 pages = {126--144}
}

@article{jones_automatic_2007,
 title = {Automatic summarising: The state of the art},
 volume = {43},
 issn = {0306-4573},
 shorttitle = {Automatic summarising},
 doi = {10.1016/j.ipm.2007.03.009},
 abstract = {This paper reviews research on automatic summarising in the last decade. This work has grown, stimulated by technology and by evaluation programmes. The paper uses several frameworks to organise the review, for summarising itself, for the factors affecting summarising, for systems, and for evaluation. The review examines the evaluation strategies applied to summarising, the issues they raise, and the major programmes. It considers the input, purpose and output factors investigated in recent summarising research, and discusses the classes of strategy, extractive and non-extractive, that have been explored, illustrating the range of systems built. The conclusions drawn are that automatic summarisation has made valuable progress, with useful applications, better evaluation, and more task understanding. But summarising systems are still poorly motivated in relation to the factors affecting them, and evaluation needs taking much further to engage with the purposes summaries are intended to serve and the contexts in which they are used. (C) 2007 Published by Elsevier Ltd.},
 language = {English},
 number = {6},
 journal = {Information Processing \& Management},
 author = {Jones, Karen Spaerck},
 month = nov,
 year = {2007},
 note = {{WOS}:000249742500004},
 keywords = {abstraction natural   language processing, abstracts, articles, evaluation, sentence extraction, summarization},
 pages = {1449--1481}
}

@article{salton_automatic_1997,
 title = {Automatic text structuring and summarization},
 volume = {33},
 issn = {0306-4573},
 doi = {10.1016/S0306-4573(96)00062-3},
 abstract = {In recent years, information retrieval techniques have been used for automatic generation of semantic hypertext links. This study applies the ideas from the automatic link generation research to attack another important problem in text processing-automatic text summarization. An automatic ''general purpose'' text summarization tool would be of immense utility in this age of information overload. Using the techniques used (by most automatic hypertext link generation algorithms) for inter-document link generation, we generate intra-document links between passages of a document. Based on the intra-document linkage pattern of a text, we characterize the structure of the text. We apply the knowledge of text structure to do automatic text summarization by passage extraction. We evaluate a set of fifty summaries generated using our techniques by comparing them to paragraph extracts constructed by humans. The automatic summarization methods perform well, especially in view of the fact that the summaries generated by two humans for the same article are surprisingly dissimilar. (C) 1997 Elsevier Science Ltd.},
 language = {English},
 number = {2},
 journal = {Information Processing \& Management},
 author = {Salton, G. and Singhal, A. and Mitra, M. and Buckley, C.},
 month = mar,
 year = {1997},
 note = {{WOS}:A1997WU98800006},
 pages = {193--207}
}

@incollection{silla_automatic_2004,
 address = {Berlin},
 title = {Automatic text summarization with genetic algorithm-based attribute selection},
 volume = {3315},
 isbn = {3-540-23806-9},
 abstract = {The task of automatic text summarization consists of generating a summary of the original text that allows the user to obtain the main pieces of information available in that text, but with a much shorter reading time. This is an increasingly important task in the current era of information overload. given the huge amount of text available in documents. In this paper the automatic text summarization is cast as a classification (supervised learning) problem, so that machine learning-oriented classification methods are used to produce summaries for documents based on a set of attributes describing those documents. The goal of the paper is to investigate the effectiveness of Genetic Algorithm ({GA})-based attribute selection in improving the performance of classification algorithms solving the automatic text summarization task. Computational results are reported for experiments with a document base formed by news extracted from The Wall Street Journal of the {TIPSTER} collection-a collection that is often used as a benchmark in the text summarization literature.},
 language = {English},
 booktitle = {Advances in Artificial Intelligence - Iberamia 2004},
 publisher = {Springer-Verlag Berlin},
 author = {Silla, C. N. and Pappa, G. L. and Freitas, A. A. and Kaestner, C. a. A.},
 editor = {Lemaitre, C. and Reyes, C. A. and Gonzalez, J. A.},
 year = {2004},
 note = {{WOS}:000226646200031},
 pages = {305--314}
}
